#!/usr/bin/env python3
"""
Omegaæ›´æ–°ç³»ç»Ÿä»£ç åº“å…¨é¢å®¡è®¡
è¯†åˆ«å’Œåˆ†ç±»å¯èƒ½éœ€è¦æ¸…ç†æˆ–æ•´åˆçš„æ–‡ä»¶
"""

import sys
import json
import ast
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Set, Tuple

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))


class CodebaseAuditor:
    """ä»£ç åº“å®¡è®¡å™¨"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.audit_results = {
            "obsolete_scripts": [],
            "redundant_docs": [],
            "legacy_configs": [],
            "test_files": [],
            "temporary_files": [],
            "reports_analysis": {},
            "config_analysis": {},
            "recommendations": {}
        }
    
    def scan_scripts_directory(self):
        """æ‰«æscriptsç›®å½•ï¼Œè¯†åˆ«è¿‡æ—¶çš„è„šæœ¬"""
        print("ğŸ” æ‰«æ /scripts ç›®å½•...")
        
        scripts_dir = self.project_root / "scripts"
        if not scripts_dir.exists():
            return
        
        # å®šä¹‰è„šæœ¬ç±»åˆ«
        migration_scripts = []
        test_scripts = []
        utility_scripts = []
        obsolete_scripts = []
        
        for script_file in scripts_dir.glob("*.py"):
            try:
                with open(script_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # åˆ†æè„šæœ¬ç”¨é€”
                script_info = {
                    "file": str(script_file.relative_to(self.project_root)),
                    "size": script_file.stat().st_size,
                    "purpose": self._analyze_script_purpose(content),
                    "imports": self._extract_imports(content),
                    "last_modified": datetime.fromtimestamp(script_file.stat().st_mtime).isoformat()
                }
                
                # åˆ†ç±»è„šæœ¬
                if any(keyword in script_file.name.lower() for keyword in 
                       ['migrate', 'cleanup', 'integrate', 'legacy']):
                    migration_scripts.append(script_info)
                elif any(keyword in script_file.name.lower() for keyword in 
                         ['test', 'verify', 'check']):
                    test_scripts.append(script_info)
                elif any(keyword in script_file.name.lower() for keyword in 
                         ['demo', 'temp', 'fix']):
                    obsolete_scripts.append(script_info)
                else:
                    utility_scripts.append(script_info)
                
            except Exception as e:
                print(f"   âŒ åˆ†æè„šæœ¬å¤±è´¥: {script_file.name}: {e}")
        
        self.audit_results["migration_scripts"] = migration_scripts
        self.audit_results["test_scripts"] = test_scripts
        self.audit_results["utility_scripts"] = utility_scripts
        self.audit_results["obsolete_scripts"] = obsolete_scripts
        
        print(f"   ğŸ“Š å‘ç°è„šæœ¬: è¿ç§»({len(migration_scripts)}) æµ‹è¯•({len(test_scripts)}) å·¥å…·({len(utility_scripts)}) è¿‡æ—¶({len(obsolete_scripts)})")
    
    def _analyze_script_purpose(self, content: str) -> str:
        """åˆ†æè„šæœ¬ç”¨é€”"""
        content_lower = content.lower()
        
        if "legacy" in content_lower and "cleanup" in content_lower:
            return "legacy_cleanup"
        elif "migrate" in content_lower:
            return "migration"
        elif "test" in content_lower and "verify" in content_lower:
            return "testing"
        elif "demo" in content_lower:
            return "demonstration"
        elif "fix" in content_lower:
            return "bug_fix"
        elif "deploy" in content_lower:
            return "deployment"
        elif "diagnose" in content_lower:
            return "diagnostics"
        else:
            return "utility"
    
    def _extract_imports(self, content: str) -> List[str]:
        """æå–è„šæœ¬çš„å¯¼å…¥è¯­å¥"""
        try:
            tree = ast.parse(content)
            imports = []
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        imports.append(alias.name)
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        imports.append(node.module)
            return imports
        except:
            return []
    
    def scan_documentation_files(self):
        """æ‰«ææ–‡æ¡£æ–‡ä»¶ï¼Œè¯†åˆ«å†—ä½™æˆ–è¿‡æ—¶çš„æ–‡æ¡£"""
        print("\nğŸ” æ‰«ææ–‡æ¡£æ–‡ä»¶...")
        
        # æ‰«ææ ¹ç›®å½•çš„markdownæ–‡ä»¶
        root_docs = []
        for md_file in self.project_root.glob("*.md"):
            doc_info = self._analyze_documentation(md_file)
            root_docs.append(doc_info)
        
        # æ‰«ædocsç›®å½•
        docs_dir_files = []
        docs_dir = self.project_root / "docs"
        if docs_dir.exists():
            for md_file in docs_dir.glob("*.md"):
                doc_info = self._analyze_documentation(md_file)
                docs_dir_files.append(doc_info)
        
        # æ‰«ædeploymentç›®å½•
        deployment_docs = []
        deployment_dir = self.project_root / "deployment"
        if deployment_dir.exists():
            for md_file in deployment_dir.glob("*.md"):
                doc_info = self._analyze_documentation(md_file)
                deployment_docs.append(doc_info)
        
        self.audit_results["root_docs"] = root_docs
        self.audit_results["docs_dir_files"] = docs_dir_files
        self.audit_results["deployment_docs"] = deployment_docs
        
        # è¯†åˆ«å†—ä½™æ–‡æ¡£
        self._identify_redundant_docs(root_docs + docs_dir_files + deployment_docs)
        
        print(f"   ğŸ“Š å‘ç°æ–‡æ¡£: æ ¹ç›®å½•({len(root_docs)}) docsç›®å½•({len(docs_dir_files)}) deploymentç›®å½•({len(deployment_docs)})")
    
    def _analyze_documentation(self, md_file: Path) -> Dict:
        """åˆ†ææ–‡æ¡£æ–‡ä»¶"""
        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            return {
                "file": str(md_file.relative_to(self.project_root)),
                "size": md_file.stat().st_size,
                "lines": len(content.split('\n')),
                "topics": self._extract_doc_topics(content),
                "mentions_legacy": "legacy" in content.lower() or "simplified" in content.lower(),
                "mentions_version_numbers": any(pattern in content.lower() for pattern in 
                                               ["version number", "1.0.0", "2.1.3", "ç‰ˆæœ¬å·"]),
                "last_modified": datetime.fromtimestamp(md_file.stat().st_mtime).isoformat()
            }
        except Exception as e:
            return {
                "file": str(md_file.relative_to(self.project_root)),
                "error": str(e)
            }
    
    def _extract_doc_topics(self, content: str) -> List[str]:
        """æå–æ–‡æ¡£ä¸»é¢˜"""
        topics = []
        content_lower = content.lower()
        
        topic_keywords = {
            "deployment": ["deploy", "éƒ¨ç½²", "å®‰è£…"],
            "usage": ["usage", "ä½¿ç”¨", "æ“ä½œ"],
            "api": ["api", "æ¥å£"],
            "troubleshooting": ["troubleshoot", "æ•…éšœ", "é—®é¢˜"],
            "setup": ["setup", "é…ç½®", "è®¾ç½®"],
            "migration": ["migrate", "è¿ç§»", "è½¬æ¢"],
            "cleanup": ["cleanup", "æ¸…ç†"],
            "legacy": ["legacy", "æ—§ç‰ˆ", "é—ç•™"]
        }
        
        for topic, keywords in topic_keywords.items():
            if any(keyword in content_lower for keyword in keywords):
                topics.append(topic)
        
        return topics
    
    def _identify_redundant_docs(self, all_docs: List[Dict]):
        """è¯†åˆ«å†—ä½™æ–‡æ¡£"""
        redundant_groups = []
        
        # æŒ‰ä¸»é¢˜åˆ†ç»„
        topic_groups = {}
        for doc in all_docs:
            if "topics" in doc:
                for topic in doc["topics"]:
                    if topic not in topic_groups:
                        topic_groups[topic] = []
                    topic_groups[topic].append(doc)
        
        # è¯†åˆ«å¯èƒ½å†—ä½™çš„ç»„
        for topic, docs in topic_groups.items():
            if len(docs) > 1:
                redundant_groups.append({
                    "topic": topic,
                    "files": [doc["file"] for doc in docs],
                    "total_size": sum(doc.get("size", 0) for doc in docs)
                })
        
        self.audit_results["redundant_doc_groups"] = redundant_groups
    
    def analyze_reports_directory(self):
        """åˆ†æreportsç›®å½•"""
        print("\nğŸ” åˆ†æ /reports ç›®å½•...")
        
        reports_dir = self.project_root / "reports"
        if not reports_dir.exists():
            self.audit_results["reports_analysis"] = {"exists": False}
            return
        
        reports = []
        total_size = 0
        
        for report_file in reports_dir.glob("*.json"):
            try:
                with open(report_file, 'r', encoding='utf-8') as f:
                    report_data = json.load(f)
                
                file_size = report_file.stat().st_size
                total_size += file_size
                
                report_info = {
                    "file": report_file.name,
                    "size": file_size,
                    "type": self._classify_report(report_file.name, report_data),
                    "date": report_data.get("date", report_data.get("cleanup_date", 
                                          report_data.get("verification_date", "unknown"))),
                    "purpose": self._determine_report_purpose(report_file.name)
                }
                reports.append(report_info)
                
            except Exception as e:
                print(f"   âŒ åˆ†ææŠ¥å‘Šå¤±è´¥: {report_file.name}: {e}")
        
        self.audit_results["reports_analysis"] = {
            "exists": True,
            "total_files": len(reports),
            "total_size": total_size,
            "reports": reports,
            "archival_candidates": [r for r in reports if r["type"] in ["migration", "cleanup", "verification"]]
        }
        
        print(f"   ğŸ“Š å‘ç°æŠ¥å‘Š: {len(reports)} ä¸ªæ–‡ä»¶ï¼Œæ€»å¤§å° {total_size} å­—èŠ‚")
    
    def _classify_report(self, filename: str, data: Dict) -> str:
        """åˆ†ç±»æŠ¥å‘Šç±»å‹"""
        filename_lower = filename.lower()
        
        if "cleanup" in filename_lower:
            return "cleanup"
        elif "migration" in filename_lower:
            return "migration"
        elif "verification" in filename_lower:
            return "verification"
        elif "test" in filename_lower:
            return "testing"
        elif "legacy" in filename_lower:
            return "legacy"
        else:
            return "operational"
    
    def _determine_report_purpose(self, filename: str) -> str:
        """ç¡®å®šæŠ¥å‘Šç”¨é€”"""
        purposes = {
            "cleanup": "è®°å½•ç³»ç»Ÿæ¸…ç†è¿‡ç¨‹",
            "migration": "è®°å½•æ•°æ®è¿ç§»è¿‡ç¨‹", 
            "verification": "è®°å½•ç³»ç»ŸéªŒè¯ç»“æœ",
            "testing": "è®°å½•æµ‹è¯•ç»“æœ",
            "legacy": "è®°å½•é—ç•™ç³»ç»Ÿå¤„ç†"
        }
        
        for key, purpose in purposes.items():
            if key in filename.lower():
                return purpose
        
        return "æœªçŸ¥ç”¨é€”"
    
    def analyze_config_directory(self):
        """åˆ†æconfigç›®å½•"""
        print("\nğŸ” åˆ†æ /config ç›®å½•...")
        
        config_dir = self.project_root / "config"
        if not config_dir.exists():
            self.audit_results["config_analysis"] = {"exists": False}
            return
        
        configs = []
        
        for config_file in config_dir.glob("*"):
            if config_file.is_file():
                config_info = {
                    "file": config_file.name,
                    "size": config_file.stat().st_size,
                    "type": config_file.suffix,
                    "purpose": self._determine_config_purpose(config_file.name),
                    "essential": self._is_essential_config(config_file.name),
                    "last_modified": datetime.fromtimestamp(config_file.stat().st_mtime).isoformat()
                }
                configs.append(config_info)
        
        self.audit_results["config_analysis"] = {
            "exists": True,
            "total_files": len(configs),
            "configs": configs,
            "essential_configs": [c for c in configs if c["essential"]],
            "optional_configs": [c for c in configs if not c["essential"]]
        }
        
        print(f"   ğŸ“Š å‘ç°é…ç½®: {len(configs)} ä¸ªæ–‡ä»¶")
    
    def _determine_config_purpose(self, filename: str) -> str:
        """ç¡®å®šé…ç½®æ–‡ä»¶ç”¨é€”"""
        purposes = {
            "config.json": "ä¸»é…ç½®æ–‡ä»¶",
            "upload_config.json": "ä¸Šä¼ é…ç½®",
            "file_filter.json": "æ–‡ä»¶è¿‡æ»¤é…ç½®",
            "local_server_config.json": "æœ¬åœ°æœåŠ¡å™¨é…ç½®",
            "batch_config": "æ‰¹å¤„ç†é…ç½®ç¤ºä¾‹",
            "upload_config_sample": "ä¸Šä¼ é…ç½®ç¤ºä¾‹"
        }
        
        for key, purpose in purposes.items():
            if key in filename:
                return purpose
        
        return "æœªçŸ¥é…ç½®"
    
    def _is_essential_config(self, filename: str) -> bool:
        """åˆ¤æ–­é…ç½®æ–‡ä»¶æ˜¯å¦å¿…éœ€"""
        essential_configs = {
            "config.json",
            "upload_config.json", 
            "file_filter.json"
        }
        
        return any(essential in filename for essential in essential_configs)
    
    def generate_recommendations(self):
        """ç”Ÿæˆæ¸…ç†å»ºè®®"""
        print("\nğŸ’¡ ç”Ÿæˆæ¸…ç†å»ºè®®...")
        
        recommendations = {
            "safe_to_delete": [],
            "archive_candidates": [],
            "consolidation_opportunities": [],
            "directory_restructuring": [],
            "maintenance_strategy": []
        }
        
        # è„šæœ¬æ¸…ç†å»ºè®®
        for script in self.audit_results.get("obsolete_scripts", []):
            if any(keyword in script["file"] for keyword in ["demo", "temp", "fix"]):
                recommendations["safe_to_delete"].append({
                    "file": script["file"],
                    "reason": "æ¼”ç¤ºæˆ–ä¸´æ—¶è„šæœ¬ï¼Œå·²å®Œæˆä½¿å‘½",
                    "type": "script"
                })
        
        for script in self.audit_results.get("migration_scripts", []):
            recommendations["archive_candidates"].append({
                "file": script["file"],
                "reason": "è¿ç§»è„šæœ¬ï¼Œä¿ç•™ä½œä¸ºå†å²è®°å½•",
                "type": "script"
            })
        
        # æ–‡æ¡£æ•´åˆå»ºè®®
        for group in self.audit_results.get("redundant_doc_groups", []):
            if len(group["files"]) > 2:
                recommendations["consolidation_opportunities"].append({
                    "files": group["files"],
                    "reason": f"å¤šä¸ªæ–‡æ¡£æ¶‰åŠç›¸åŒä¸»é¢˜: {group['topic']}",
                    "type": "documentation"
                })
        
        # æŠ¥å‘Šå½’æ¡£å»ºè®®
        reports_analysis = self.audit_results.get("reports_analysis", {})
        if reports_analysis.get("exists"):
            for report in reports_analysis.get("archival_candidates", []):
                recommendations["archive_candidates"].append({
                    "file": f"reports/{report['file']}",
                    "reason": f"{report['purpose']} - å¯å½’æ¡£",
                    "type": "report"
                })
        
        # ç›®å½•é‡æ„å»ºè®®
        recommendations["directory_restructuring"] = [
            {
                "action": "åˆ›å»º /archive ç›®å½•",
                "reason": "å­˜æ”¾å†å²æ–‡ä»¶å’ŒæŠ¥å‘Š",
                "files": "è¿ç§»è„šæœ¬ã€æ¸…ç†æŠ¥å‘Šç­‰"
            },
            {
                "action": "æ•´ç† /docs ç›®å½•",
                "reason": "æŒ‰ä¸»é¢˜ç»„ç»‡æ–‡æ¡£",
                "files": "åˆå¹¶é‡å¤çš„éƒ¨ç½²å’Œä½¿ç”¨æŒ‡å—"
            }
        ]
        
        # ç»´æŠ¤ç­–ç•¥
        recommendations["maintenance_strategy"] = [
            "å®šæœŸæ¸…ç† /reports ç›®å½•ä¸­çš„æ—§æŠ¥å‘Š",
            "åˆå¹¶é‡å¤çš„æ–‡æ¡£æ–‡ä»¶",
            "ç§»é™¤å®Œæˆä½¿å‘½çš„ä¸´æ—¶è„šæœ¬",
            "ä¿æŒé…ç½®æ–‡ä»¶çš„æœ€å°åŒ–",
            "å»ºç«‹æ–‡ä»¶å‘½åè§„èŒƒ"
        ]
        
        self.audit_results["recommendations"] = recommendations
    
    def generate_audit_report(self):
        """ç”Ÿæˆå®¡è®¡æŠ¥å‘Š"""
        print("\nğŸ“„ ç”Ÿæˆå®¡è®¡æŠ¥å‘Š...")
        
        report = {
            "audit_date": datetime.now().isoformat(),
            "project_status": "simplified_system_active",
            "audit_results": self.audit_results,
            "summary": {
                "total_scripts": len(self.audit_results.get("migration_scripts", [])) + 
                               len(self.audit_results.get("test_scripts", [])) + 
                               len(self.audit_results.get("utility_scripts", [])) + 
                               len(self.audit_results.get("obsolete_scripts", [])),
                "total_docs": len(self.audit_results.get("root_docs", [])) + 
                             len(self.audit_results.get("docs_dir_files", [])) + 
                             len(self.audit_results.get("deployment_docs", [])),
                "reports_count": self.audit_results.get("reports_analysis", {}).get("total_files", 0),
                "config_count": self.audit_results.get("config_analysis", {}).get("total_files", 0)
            }
        }
        
        try:
            report_path = self.project_root / "reports" / "comprehensive_codebase_audit.json"
            report_path.parent.mkdir(exist_ok=True)
            
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… å®¡è®¡æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
            return report
            
        except Exception as e:
            print(f"âŒ ä¿å­˜å®¡è®¡æŠ¥å‘Šå¤±è´¥: {e}")
            return report
    
    def run_comprehensive_audit(self):
        """è¿è¡Œå…¨é¢å®¡è®¡"""
        print("=" * 60)
        print("ğŸ” Omegaæ›´æ–°ç³»ç»Ÿä»£ç åº“å…¨é¢å®¡è®¡")
        print("=" * 60)
        
        # æ‰§è¡Œå„é¡¹å®¡è®¡
        self.scan_scripts_directory()
        self.scan_documentation_files()
        self.analyze_reports_directory()
        self.analyze_config_directory()
        self.generate_recommendations()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self.generate_audit_report()
        
        # æ˜¾ç¤ºæ€»ç»“
        self._display_audit_summary()
        
        return report
    
    def _display_audit_summary(self):
        """æ˜¾ç¤ºå®¡è®¡æ€»ç»“"""
        print("\n" + "=" * 60)
        print("ğŸ“Š å®¡è®¡ç»“æœæ€»ç»“")
        print("=" * 60)
        
        # è„šæœ¬ç»Ÿè®¡
        migration_count = len(self.audit_results.get("migration_scripts", []))
        test_count = len(self.audit_results.get("test_scripts", []))
        utility_count = len(self.audit_results.get("utility_scripts", []))
        obsolete_count = len(self.audit_results.get("obsolete_scripts", []))
        
        print(f"\nğŸ“œ è„šæœ¬æ–‡ä»¶:")
        print(f"   è¿ç§»è„šæœ¬: {migration_count}")
        print(f"   æµ‹è¯•è„šæœ¬: {test_count}")
        print(f"   å·¥å…·è„šæœ¬: {utility_count}")
        print(f"   è¿‡æ—¶è„šæœ¬: {obsolete_count}")
        
        # æ–‡æ¡£ç»Ÿè®¡
        root_docs = len(self.audit_results.get("root_docs", []))
        docs_dir = len(self.audit_results.get("docs_dir_files", []))
        deployment_docs = len(self.audit_results.get("deployment_docs", []))
        redundant_groups = len(self.audit_results.get("redundant_doc_groups", []))
        
        print(f"\nğŸ“š æ–‡æ¡£æ–‡ä»¶:")
        print(f"   æ ¹ç›®å½•æ–‡æ¡£: {root_docs}")
        print(f"   docsç›®å½•æ–‡æ¡£: {docs_dir}")
        print(f"   deploymentæ–‡æ¡£: {deployment_docs}")
        print(f"   å†—ä½™æ–‡æ¡£ç»„: {redundant_groups}")
        
        # æŠ¥å‘Šå’Œé…ç½®ç»Ÿè®¡
        reports_analysis = self.audit_results.get("reports_analysis", {})
        config_analysis = self.audit_results.get("config_analysis", {})
        
        if reports_analysis.get("exists"):
            print(f"\nğŸ“Š æŠ¥å‘Šæ–‡ä»¶: {reports_analysis.get('total_files', 0)}")
        
        if config_analysis.get("exists"):
            essential_configs = len(config_analysis.get("essential_configs", []))
            optional_configs = len(config_analysis.get("optional_configs", []))
            print(f"\nâš™ï¸  é…ç½®æ–‡ä»¶: å¿…éœ€({essential_configs}) å¯é€‰({optional_configs})")
        
        # æ¸…ç†å»ºè®®
        recommendations = self.audit_results.get("recommendations", {})
        safe_delete = len(recommendations.get("safe_to_delete", []))
        archive_candidates = len(recommendations.get("archive_candidates", []))
        consolidation = len(recommendations.get("consolidation_opportunities", []))
        
        print(f"\nğŸ’¡ æ¸…ç†å»ºè®®:")
        print(f"   å¯å®‰å…¨åˆ é™¤: {safe_delete} ä¸ªæ–‡ä»¶")
        print(f"   å»ºè®®å½’æ¡£: {archive_candidates} ä¸ªæ–‡ä»¶")
        print(f"   æ•´åˆæœºä¼š: {consolidation} ä¸ªç»„")


def main():
    """ä¸»å‡½æ•°"""
    auditor = CodebaseAuditor()
    auditor.run_comprehensive_audit()


if __name__ == "__main__":
    main()
